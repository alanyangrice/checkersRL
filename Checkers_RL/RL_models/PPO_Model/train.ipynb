{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers_env import CheckersEnv\n",
    "from Agent import PPOAgent\n",
    "from Memory import Memory\n",
    "from checkers_game import BLUE, RED\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agents\n",
    "env = CheckersEnv()\n",
    "input_shape = (1, 8, 8)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Create Agents 1 and 2 to play checkers\n",
    "agent1 = PPOAgent(input_shape, n_actions)\n",
    "agent2 = PPOAgent(input_shape, n_actions)\n",
    "\n",
    "# Track the starting agent (agent1 as BLUE initially)\n",
    "agent1_side = BLUE\n",
    "agent2_side = RED\n",
    "\n",
    "# Data storage for monitoring training progress\n",
    "training_data = {\n",
    "    \"epoch\": [],\n",
    "    \"episode_reward\": [],\n",
    "    \"win_rate_agent1\": [],\n",
    "    \"win_rate_agent2\": [],\n",
    "    \"average_episode_length\": []\n",
    "}\n",
    "\n",
    "# Directory for saving models\n",
    "model_dir = \"PPO_saved_models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Directory for saving random games\n",
    "game_info_dir = \"saved_games\"\n",
    "if not os.path.exists(game_info_dir):\n",
    "    os.makedirs(game_info_dir)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "num_episodes = 1000  # Number of episodes per epoch\n",
    "save_interval = 1  # Save model every epoch (can adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_rewards = 0\n",
    "    total_steps = 0\n",
    "    agent1_wins, agent2_wins = 0, 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset environment for each episode\n",
    "        done = False  # Flag to check if game is over\n",
    "        memory1 = Memory()  # Memory object for agent1\n",
    "        memory2 = Memory()  # Memory object for agent2\n",
    "\n",
    "        # Track game info if sampling condition is met\n",
    "        game_info = [] if random.random() < 0.01 else None  # Log moves for ~1% of games\n",
    "        \n",
    "        # Assign agents based on sides for the current episode\n",
    "        current_agent, opponent_agent = (agent1, agent2) if agent1_side == BLUE else (agent2, agent1)\n",
    "        current_memory, opponent_memory = (memory1, memory2) if agent1_side == BLUE else (memory2, memory1)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action and log probabilities from the current agent\n",
    "            action, log_prob, _ = current_agent.select_action(state)\n",
    "            \n",
    "            # Step the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Track rewards\n",
    "            episode_reward += reward\n",
    "            total_rewards += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            # Record in memory based on current agent's perspective\n",
    "            current_memory.add(state, info[action], reward, log_prob)\n",
    "            \n",
    "            # Save selected games stats\n",
    "            if game_info is not None:\n",
    "                game_info.append({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"episode\": episode,\n",
    "                    \"turn\": env.checkers.turn,\n",
    "                    \"move\": info[\"legal_moves\"][action] if info[\"move_success\"] else None,\n",
    "                    \"reward\": reward,\n",
    "                    \"success\": info[\"move_success\"],\n",
    "                    \"winner\": info[\"winner\"]\n",
    "                })\n",
    "\n",
    "            # Track win/loss for agents\n",
    "            if done:\n",
    "                if info[\"winner\"] == \"Blue Wins!\":\n",
    "                    agent1_wins += 1 if agent1_side == BLUE else 0\n",
    "                    agent2_wins += 1 if agent2_side == BLUE else 0\n",
    "                elif info[\"winner\"] == \"Red Wins!\":\n",
    "                    agent1_wins += 1 if agent1_side == RED else 0\n",
    "                    agent2_wins += 1 if agent2_side == RED else 0\n",
    "\n",
    "            # Alternate agents\n",
    "            current_agent, opponent_agent = opponent_agent, current_agent\n",
    "            current_memory, opponent_memory = opponent_memory, current_memory\n",
    "\n",
    "            # Update state for next step\n",
    "            state = next_state\n",
    "        \n",
    "        # Save game information if sampled\n",
    "        if game_info:\n",
    "            game_info_df = pd.DataFrame(game_info)\n",
    "            game_info_df.to_csv(f\"{game_info_dir}/game_info_epoch_{epoch}_episode_{episode}.csv\", index=False)\n",
    "        \n",
    "        # Store episode data\n",
    "        total_steps += episode_steps\n",
    "        \n",
    "        # After episode ends, update each agent using its memory\n",
    "        agent1.update(memory1)\n",
    "        agent2.update(memory2)\n",
    "        \n",
    "        # Swap sides after each episode\n",
    "        agent1_side, agent2_side = agent2_side, agent1_side\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    avg_reward = total_rewards / num_episodes\n",
    "    avg_steps = total_steps / num_episodes\n",
    "    win_rate1 = agent1_wins / num_episodes\n",
    "    win_rate2 = agent2_wins / num_episodes\n",
    "\n",
    "    # Append data for this epoch\n",
    "    training_data[\"epoch\"].append(epoch)\n",
    "    training_data[\"episode_reward\"].append(avg_reward)\n",
    "    training_data[\"win_rate_agent1\"].append(win_rate1)\n",
    "    training_data[\"win_rate_agent2\"].append(win_rate2)\n",
    "    training_data[\"average_episode_length\"].append(avg_steps)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Avg Reward: {avg_reward:.2f}, Win Rate Agent1: {win_rate1:.2%}, Win Rate Agent2: {win_rate2:.2%}, Avg Steps: {avg_steps:.2f}\")\n",
    "\n",
    "    # Save model after every save_interval epochs\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save(agent1.policy.state_dict(), os.path.join(model_dir, f\"agent1_epoch_{epoch + 1}.pt\"))\n",
    "        torch.save(agent2.policy.state_dict(), os.path.join(model_dir, f\"agent2_epoch_{epoch + 1}.pt\"))\n",
    "\n",
    "# Save training data as a CSV for later analysis\n",
    "training_df = pd.DataFrame(training_data)\n",
    "training_df.to_csv(\"training_progress.csv\", index=False)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
